
<!doctype html>
<html lang="en">
  <head>
    <!-- Required meta tags -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- Bootstrap CSS -->
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">
     <link href="css/theme.css" rel="stylesheet" type="text/css" media="all">
    <title>LID Challenge CVPR 2019</title>
  </head>
  <body>
  		<div class="container">


  			<nav class="navbar navbar-expand-sm navbar-light bg-light fixed-top">
  				  <!-- Toggler/collapsibe Button -->
				  <button class="navbar-toggler navbar-toggler-right" type="button" data-toggle="collapse" data-target="#collapsibleNavbar">
				    <span class="navbar-toggler-icon"></span>
				  </button>
			    <div class="mx-auto d-sm-flex d-block flex-sm-nowrap">
				    <nav class="navbar navbar-light bg-light">
					  <a class="navbar-brand" href="#">
					    <img src="img/logo.png" width="180" height="60" alt="">
					  </a>
					</nav>
			        <div class="collapse navbar-collapse text-center" id="collapsibleNavbar" style="padding-top: 30px">
			           <ul class="navbar-nav font-weight-bold mc-auto">
		                <li class="nav-item">
		                  <a href="./index.html" class="nav-link"> Home </a>
		                </li>
		                <li class="nav-item dropdown">

		                  <a href="#challenge" class="nav-link">Challenge</a>
		                  <!--

		                   <a class="nav-link dropdown-toggle" href="#" id="componentsDropdown" role="button" data-toggle="dropdown" aria-expanded="false">Challenge</a>
		                    <div class="dropdown-menu" aria-labelledby="componentsDropdown">

		                    <a class="dropdown-item" href="#challenge">Challenge Tracks</a>

		                    <a class="dropdown-item" href="#challenge">Data and Evaluation</a>

		                    <a class="dropdown-item" href="#challenge">Challenge Awards</a>

		                  </div>-->
		                </li>
		                <li class="nav-item">
		                  <a href="#papers" class="nav-link"> Call for papers </a>
		                </li>
		                <li class="nav-item">
		                  <a href="#dates" class="nav-link"> Important Dates </a>
		                </li>
		                <li class="nav-item">
		                  <a href="#schedule" class="nav-link"> Schedule </a>
		                </li> 
		                <li class="nav-item">
		                  <a href="#speakers" class="nav-link"> Invited Speakers </a>
		                </li>  
		                <li class="nav-item">
		                  <a href="#accepted" class="nav-link"> Accepted papers</a>
		                </li>
		                <li class="nav-item">
		                  <a href="#organizers" class="nav-link"> Organizers </a>
		                </li> 
		                <li class="nav-item">
		                  <a href="../" class="nav-link"> LID 2020 </a>
		                </li> 
	              	</ul> 
			        </div>
			    </div>
			</nav>
		</div> 

	<section id="home" class="mb-5">	
		<div class="carousel slide" data-ride="carousel">
		  <div class="carousel-inner">
		    <div class="carousel-item active">
		      <img class="d-block w-100" src="img/bkg2.jpg" alt="Long Beach">
		      <div class="carousel-caption titlebox">
			    <p class="display-4 font-weight-bold text-shadow"> Weakly Supervised Learning for Real-World Computer Vision Applications and the 1st Learning from Imperfect Data (LID) Challenge</p>
			    <p> CVPR 2019 Workshop, Long Beach, CA</p>
			  </div>
		    </div> 
		  </div>  
		</div>
	</section> 

	<section id="introduction" class="py-6 bg-white">
	      <div class="container">
	      	<p><strong>The workshop will be held on Sunday (June 16 2019, Los Angeles Time) at  Hyatt Regency B Long Beach. Poster: June 16 2019,10:00-11:00 AM, Pacific Arena Ballroom (main convention center)</strong> </p>
	        <div class="row mb-5 col-12"> 
	          	<h2>Introduction</h2>  
	            <p>Weakly supervised learning refers to a variety of studies that attempt to address the challenging pattern recognition tasks by learning from weak or imperfect supervision. Supervised learning methods including Deep Convolutional Neural Networks (DCNNs) have significantly improved the performance in many problems in the field of computer vision, thanks to the rise of large-scale annotated data set and the advance in computing hardware. However, these supervised learning approaches are notorious "data hungry", which makes them are sometimes not practical in many real-world industrial applications. We are often facing the problem that we are not able to acquire enough amount of perfect annotations (e.g., object bounding boxes and pixel-wise masks) for reliable training models. To address this problem, many efforts in so-called weakly supervised learning approaches have been made to improve the DCNNs training to deviate from traditional paths of supervised learning using imperfect data. For instance, various approaches have proposed new loss functions or novel training schemes. Weakly supervised learning is a popular research direction in Computer Vision and Machine Learning communities, many research works have been devoted to related topics, leading to rapid growth of related publications in the top-tier conferences and journals such as CVPR, ICCV, ECCV, T-IP, and T-PAMI. We organize this workshop to investigate current ways of building industry level AI system relying on learning from imperfect data. We hope this workshop will attract attention and discussions from both industry and academic people.</p>
	            
	          </div>  
	    </div>
	</section>
     
	<section id="papers" class="py-6 mb-5 bg-white" style="padding-top:80px;margin-top:-80px;">
	      <div class="container">
	        <div class="row col-12"> 
	          	<h2>Call for Papers</h2> <br>
            </div>
            <div class="row col-12"> 
	            <p>The workshop are expected to deal with data in a weakly supervised manner, which will cover but not limit to the following topics: </p>
	            <ul>
	            	<li>Weakly supervised learning algorithms</li>
	            	<li>One/few shot learning for computer vision</li>
	            	<li>Learning with noisy webly data</li>
	            	<li>Weakly supervised learning for medical images</li>
	            	<li>Real world image applications, e.g. object semantic segmentation/detection/localization, scene parsing, etc.</li>
	            	<li>Real world video applications, e.g. action recognition, event detection, object tracking,  video object detection/segmentation, etc.</li>
	            	<li>New datasets and metrics to evaluate the benefit of the weakly supervised approaches for the specific vision problems.</li>
	            </ul> 
			</div>
			<p class="col-12">
				<strong> Format </strong> : Papers are limited to 8 pages, including figures and tables, in the CVPR style. Additional pages containing only cited references are allowed. <br>
				<strong> Location:</strong> Long Beach, United States <br>
				<strong> Submission Site:</strong> <a href="https://cmt3.research.microsoft.com/LID2019/Submission/Index">https://cmt3.research.microsoft.com/LID2019/Submission/Index </a><br>
				<strong> Latex/Word Templates: </strong> <a href="http://cvpr2019.thecvf.com/files/cvpr2019AuthorKit.zip"> http://cvpr2019.thecvf.com/files/cvpr2019AuthorKit.zip </a><br>
				<strong> <span class="text-danger">*Note:</span> </strong> Paper should be prepared in blind-submission review-formatted template. <br>
			 </p>
	    </div>
	</section>


	<section id="challenge" class="py-6 mb-5 bg-white" style="padding-top:80px;margin-top:-80px;">
	      <div class="container">
	      	<strong> <span class="text-danger">*Note: The challenge deadline is postponed to Jun 9, 2019 7:59:59 AM CST. (2019-05-30)  </span> </strong> <br/> 
	        <div class="row col-12">  
	          	<h2>Challenge</h2> 
	            <p>We will organize the first Learning from Imperfect Data (LID) challenge on object semantic segmentation and scene parsing, which includes two competition tracks（<strong>challenge deadline: June 8, 2019</strong>）:  </p>
            </div>
            <div class="row col-12">  
	            <h4>Track1: Object semantic segmentation with image-level supervision</h4>
	            <p> In this track, image-level annotations are provided for supervision and the target is performing pixel-level classification. The dataset is built upon the image detection track of ImageNet Large Scale Visual Recognition Competition (ILSVRC). We provide pixel-level annotations of 15K images (validation/testing: 5K/10K) from 200 basic-level categories for evaluation. To the best of our knowledge, this is the most diverse dataset to evaluate the semantic segmentation in the weakly supervised manner. <br/
	            </p>
				<div class="row col-12">
					<ul>
						<li> For training, all the images in the training set of ILSVRC DET are permitted. </li>
						<li> If supervised saliency detection is applied, only MSRA-B dataset is permitted.  </li>
						<li> The training dataset is available at  <a href="http://image-net.org/image/ILSVRC2017/ILSVRC2017_DET.tar.gz">Imagenet DET</a> , val and test dataset are available at <a href="https://pan.baidu.com/s/1_rzQNkTEFmTJdiYYbhySSQ">Baidu Drive </a> and <a href="https://drive.google.com/open?id=1B0enLzxyIULbRZWUi0XpNnXCu7nwFI-f"> Google Drive</a> </li>
						<li> Evaluate:  <a href="https://evalai.cloudcv.org/web/challenges/challenge-page/276/overview">https://evalai.cloudcv.org/web/challenges/challenge-page/276/overview</a>  </li>
					</ul>
			    </div>

            </div>

            <div class="row col-12"> 
	            <h4>Track2: Scene parsing with point-based supervision</h4>
	            <p> Beyond object segmentation, background categories such as wall, road, sky need to be further specified for the scene parsing, which is a challenging task compared with object semantic segmentation. Thus, it will be more difficult and expensive to manually annotate pixel-level mask for this task. In this track, we propose to leverage several labeled points that are much easier to obtain to guide the training process.
The dataset is built upon the well-known ADE20K, which includes 20,210 training images from 150 categories. We provide the point-based annotations on the training set. Please download the data from <a href="http://sceneparsing.csail.mit.edu/data/LID2019"> LID Challenge Track2 data </a>. <br/>
	            <strong> <span class="text-danger">*Note:</span> </strong>
				<div class="row col-12">
	            <ul>
					<li>Only point-based annotations on the training set (20K) can be used for training, dense annotaions are <strong>NOT</strong> permitted;</li>
					<li>The prediction on 150 classes should have index range [0, 149] inclusive;</li>
					<li>Participants can use dense annotations on the validation set (2K) for model tuning;</li>
					<li>Challenge evaluation is performed on the testing (>3K) sets.</li>
					<li>Evaluate: <a href="https://evalai.cloudcv.org/web/challenges/challenge-page/300/overview">https://evalai.cloudcv.org/web/challenges/challenge-page/300/overview</a></li>
				</ul>
			    </div>
	            </p>
           </div>
	         </div>  
	    </div>
	</section>


	<section id="dates" class="py-6 mb-5 bg-gray-100" style="padding-top:80px;margin-top:-80px;"> 
     	<div class="container">
       	 	<div class="row col-12"> 
           		 <h2>Important Dates</h2>
           		 <table class="table table-striped">
           		 	<thead> 
				  	<tr>
				      <th scope="col"> Description </th>
				      <th scope="col"> Date </th> 
				    </tr>
					</thead>
			      <tbody>
			        <tr> <td>Paper Submission Deadline</td> <td>May 1, 2019</td> </tr>
			        <tr> <td>Notification to Authors</td> <td>May 7, 2019</td> </tr>
			        <tr> <td>Camera-Ready Deadline</td> <td>June 1, 2019</td> </tr> 
			        <tr> <td>Challenge Deadline</td> <td>June 8, 2019</td> </tr>
			        <tr> <td>Poster </td> <td>10:00-11:00, June 16, 2019</td> </tr>
			      </tbody>
			    </table>
			</div>
		</div>
	</section>

	<section id="schedule" class="py-6 mb-5 bg-white" style="padding-top:80px;margin-top:-80px;"> 
     	<div class="container">
       	 	<div class="row col-12"> 
           		 <h2>Workshop Schedule</h2>
           		 <table class="table table-striped">
				  <thead> 
				  	<tr>
				      <th scope="col"> Time </th>
				      <th scope="col"> Description </th> 
				    </tr>
				  </thead>
				  <tbody>
				  	<tr><td>08:20-08:30</td>	<td>Opening remarks and welcome and the LID challenge summary</td> </tr>
					<tr><td>08:30-09:10</td>	<td><strong>James Thewlis</strong>, PhD student, advised by Andrea Vedaldi, University of Oxford<br/>
					                                 "Learning from motion with little or no supervision"</td> </tr>
					<tr><td>09:20-10:00</td>	<td><strong>Ming-Hsuan Yang</strong> , "Unseen Object Segmentation in Videos via Transferable Representations"
</td> </tr> 
					<tr><td>10:00-11:00</td>	<td>Poster, Pacific Arena Ballroom (main convention center)</td> </tr>
					<tr><td>10:00-10:30</td>	<td>coffee break</td> </tr>
					<tr><td>10:30-11:00</td>	<td><strong>Bolei Zhou</strong> , Assistant Professor, The Chinese University of Hong Kong<br/>
					                                "Objects Disentangled from Classifying and Synthesizing Scenes"</td> </tr> 
					<tr><td>11:00-11:30</td>	<td><strong> Yanping Huang</strong>, Software Engineer, Google Brain<br/>
					                                "GPipe: Easy Scaling with Micro-Batch Pipeline Parallelism"</td> </tr> 
					<tr><td>11:30-12:00</td>	<td><strong>Anurag Arnab</strong> , PhD student, University of Oxford<br/>
					                                "Learning from Weak Supervision: Panoptic Segmentation and 3D Human Pose Estimation"</td> </tr>
					<tr><td>12:00-14:00</td>	<td>Lunch</td> </tr>
					<tr><td>14:00-14:30</td>	<td><strong>Rogerio Feris</strong>, Research Manager, IBM<br/>
					                                "Learning More from Less: Weak Supervision and Beyond"</td> </tr>
					<tr><td>14:30-15:00</td>	<td><strong>Bernardino Romera-Paredes</strong>, Research scientist, Google Deepmind<br/>
					                                "Enhancing U-Nets to Segment Ambiguous Images"</td> </tr>
					<tr><td>15:00-15:20</td>	<td>coffee break</td> </tr>
					<tr><td>15:20-15:40</td>	<td>Oral talk 1: Winner of Track1: Object semantic segmentation with image-level supervision</td> </tr>
					<tr><td>15:40-16:00</td>	<td>Oral talk 2: Winner of Track2: Scene parsing with point-level supervision</td> </tr> 
					<tr><td>16:00-16:15</td>	<td>Awards & Future Plans</td> </tr> 
				  </tbody>
				</table>
			</div>
		</div>
	</section>

	<section id="accepted" class="py-6 mb-5 bg-gray-100" style="padding-top:80px;margin-top:-80px;"> 
	     	<div class="container">
	       	 	<div class="row col-12"> 
	       	 		 <h2>2019 Accepted papers</h2>
           		 <table class="table table-striped">
	       	 		 <tbody>
			        <tr> <td>#74</td> <td>Missing Labels in Object Detection</td> </tr>
					<tr> <td>#75</td> <td> Utilizing the Instability in Weakly Supervised Object Detection</td> </tr>
					<tr> <td>#76</td> <td> Dense Crowd Counting Convolutional Neural Networks with Minimal Data using Semi-Supervised Dual-Goal Generative Adversarial Networks</td> </tr>
					<tr> <td>#77</td> <td> A Dual Attention Network with Semantic Embedding for Few-shot Learning</td> </tr>
					<tr> <td>#78</td> <td> A pairwise learning strategy for video-based face recognition </td> </tr>
					<tr> <td>#79</td> <td> Semantic Part RCNN for Real-World Pedestrian Detection</td> </tr>
					<tr> <td>#80</td> <td> Semi-supervised learning based on generative adversarial network: a comparison between good GAN and bad GAN approach</td> </tr>
					<tr> <td>#81</td> <td> Class Subset Selection for Partial Domain Adaptation</td> </tr>
					<tr> <td>#82</td> <td> Self-supervised Difference Detection for Refinement CRF and Seed Interpolation</td> </tr>
					</tbody>
					</table>
				</div>
			</div>
	</section>
	  <section id="speakers" class="py-6 mb-5 bg-white" style="padding-top:80px;margin-top:-80px;">
	      <div class="container ">
	          	<div class="row col-12">  <h2>Invited Speakers</h2> </div>
	        <div class="row text-center justify-content-between">  
                <div class="col-2"> 
                	<a href="http://www.robots.ox.ac.uk/~vedaldi//">
				      <img class="portrait" src="img/vedaldi.jpg" />
				    </a>
					   
					<a href="http://www.robots.ox.ac.uk/~vedaldi//"> Andrea Vedaldi</a> 
				    <h6> Associate Professor, University of Oxford </h6>
					 
				</div>  

				<div class="col-2"> 
                	<a href="http://faculty.ucmerced.edu/mhyang/">
				      <img class="portrait" src="img/mhyang2005_70.jpg" />
				    </a>
					 
					<a href="http://faculty.ucmerced.edu/mhyang/">Ming-Hsuan Yang</a>
				    <h6> Research Scientist, Google AI </h6>	 
				</div>

				<div class="col-2"> 
                	<a href="http://rogerioferis.com/">
				      <img class="portrait" src="img/FerisPic.png" />
				    </a> 
					<a href="http://rogerioferis.com/">Rogerio Feris</a>  
				    <h6> Research Manager, IBM T.J. Watson Research Center </h6>	 
				</div>
 
				<div class="col-2"> 
                	<a href="https://sites.google.com/site/jshfeng/">
				      <img class="portrait" src="img/fjia.jpg" />
				    </a> 
					<a href="https://sites.google.com/site/jshfeng/"> Jiashi Feng</a>  
				    <h6> Assistant Professor, National University of Singapore </h6>	 
				</div>
			</div>


			<div class="row text-center mb-5 justify-content-between">  
 
				<div class="col-2"> 
                	<a href="http://people.csail.mit.edu/bzhou/">
				      <img class="portrait" src="img/zhoubolei.jpg" />
				    </a> 
					<a href="http://people.csail.mit.edu/bzhou/">Bolei Zhou</a>
				    <h6>Assistant Professor, The Chinese University of Hong Kong</h6> 
				</div>

				<div class="col-2"> 
                	<a href="https://www.linkedin.com/in/huangyp">
				      <img class="portrait" src="img/huangyangping.jpg" />
				    </a> 
					<a href="https://www.linkedin.com/in/huangyp/">Yanping Huang</a>
				    <h6>Software Engineer, Google Brain</h6> 
				</div>
               
				 
				<div class="col-2"> 
                	<a href="http://romera-paredes.com/">
				      <img class="portrait" src="img/miniyo.jpg" />
				    </a> 

					<a href="http://romera-paredes.com/">Bernardino Romera-Paredes</a>
				    <h6>Research Scientist, Google Deepmind</h6>	 
				</div>
			  
				<div class="col-2"> 
                	<a href="http://www.robots.ox.ac.uk/~aarnab/">
				      <img class="portrait" src="img/anurag.jpg" />
				    </a>
					 
					<a href="http://www.robots.ox.ac.uk/~aarnab/">Anurag Arnab</a>
				    <h6> PhD Student, University of Oxford </h6>					
					 
				</div>
			</div>
	    </div>
	</section>  

	  <section id="organizers" class="py-6 mb-5 bg-white" style="padding-top:80px;margin-top:-80px;">
	      <div class="container ">
	          	<div class="row col-12">  <h2>Organizers</h2> </div>
	        <div class="row text-center justify-content-between">  
                <div class="col-2"> 
                	<a href="https://weiyc.github.io/">
				      <img class="portrait" src="img/wyc2.jpg" />
				    </a> 
					<a href="https://weiyc.github.io/">Yunchao Wei</a> 
				</div>
 
				<div class="col-2"> 
                	<a href="https://kylezheng.org/">
				      <img class="portrait" src="img/shuai_zheng.jpg" />
				    </a> 
					<a href="https://kylezheng.org/">Shuai (Kyle) Zheng</a> 
				</div>
 
				<div class="col-2"> 
                	<a href="https://mmcheng.net/cmm/">
				      <img class="portrait" src="img/CMMC.jpg" />
				    </a> 
					<a href="https://mmcheng.net/cmm/"> Ming-Ming Cheng</a>   
				</div>
 
				<div class="col-2"> 
                	<a href="https://www.cs.cmu.edu/~xiaodan1/">
				      <img class="portrait" src="img/xiaodan.jpg" />
				    </a>
					  
					<a href="https://www.cs.cmu.edu/~xiaodan1/">Xiaodan Liang</a>  
				</div>
			</div>
			<div class="row text-center justify-content-between">  

				<div class="col-2"> 
                	<a href="http://www.mit.edu/~hangzhao/">
				      <img class="portrait" src="img/hangzhao.jpg" />
				    </a> 
					<a href="http://www.mit.edu/~hangzhao/">Hang Zhao</a> 
				 
				</div> 

				<div class="col-2"> 
                	<a href="https://ece.illinois.edu/directory/profile/hshi10">
				      <img class="portrait" src="img/honghuishi.jpg" />
				    </a>
					 
					<a href="https://ece.illinois.edu/directory/profile/hshi10">Honghui Shi</a>  
				</div> 
				<div class="col-2"> 
                	<a href="#">
				      <img class="portrait" src="img/Liwei.JPG" />
				    </a> 

					<a href="#">Liwei Wang</a>  
				</div> 
	            <div class="col-2"> 
                	<a href="http://web.mit.edu/torralba/www/">
				      <img class="portrait" src="img/antonioTorralbaS.jpg" />
				    </a> 

					<a href="http://web.mit.edu/torralba/www/">Antonio Torralba</a>  
				</div> 
	        </div> 
	        <div class="row mb-5 text-center justify-content-between"> 
				<div class="col-2"> 
                	<a href="https://ece.illinois.edu/directory/profile/t-huang1">
				      <img class="portrait" src="img/tomashuang.jpg" />
				    </a> 

					<a href="https://ece.illinois.edu/directory/profile/t-huang1">Thomas Huang</a>  
				</div>
	        </div>
	    </div>
	</section>  
 
	  <section id="organizers" class="py-6 mb-5 bg-white" style="padding-top:80px;margin-top:-80px;">
	      <div class="container ">
	          	<div class="row col-12">  <h2>Webmaster</h2> </div> 
			<div class="row text-center justify-content-between">  

				<div class="col-2"> 
                	 
				      <img class="portrait" src="img/linzheng.jpg" />  Zheng Lin
				</div> 

				<div class="col-2">  
				      <img class="portrait" src="img/liuting.jpg" /> Ting Liu
				</div> 

				<div class="col-2">   </div> 
				<div class="col-2">   </div>
	            
	        </div> 
	    </div>
	</section>  

	  <section id="contact" class="py-6 mb-5 bg-white" style="padding-top:80px;margin-top:-80px;">
	      <div class="container">
	        <div class="row col-12"> 
	          	<h2>Contact</h2>
	         </div>
             <div class="row col-12"> 
	            <p> yunchao@illinois.edu, szhengcvpr@gmail.com  </p> 
	          </div>
	           
	    </div>
	</section>
  
<script>
  $(document).ready(function() {
    $('.carousel .carousel-caption').css('zoom', $('.carousel').width()/1250);
  });

  $(window).resize(function() {
    $('.carousel .carousel-caption').css('zoom', $('.carousel').width()/1250);
  });
</script>
    <!-- Optional JavaScript -->
    <!-- jQuery first, then Popper.js, then Bootstrap JS -->
    <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js" integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1" crossorigin="anonymous"></script>
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js" integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM" crossorigin="anonymous"></script>
  </body>
</html>